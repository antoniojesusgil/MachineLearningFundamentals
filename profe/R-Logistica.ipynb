{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Ejercicio Wisconsin </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wisconsin\n",
    "import sklearn.datasets\n",
    "wisconsin = sklearn.datasets.load_breast_cancer()\n",
    "wisconsin.keys()\n",
    "wisconsin_data = wisconsin['data']\n",
    "wisconsin_target = wisconsin['target']\n",
    "\n",
    "# creamos un objeto con los parámetros por defecto\n",
    "#lr = LogisticRegression(solver='lbfgs', tol = 0.1)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "lr.fit(wisconsin_data,wisconsin_target)\n",
    "\n",
    "# obtenemos una predicción para los datos de pima\n",
    "wisconsin_prediction = lr.predict(wisconsin_data)\n",
    "\n",
    "# comparamos estos datos con el error cuadrático medio\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('Error cuadrático medio:')\n",
    "print(mean_squared_error(wisconsin_target, wisconsin_prediction))\n",
    "\n",
    "# y obtenemos directamente el score\n",
    "print('Coeficiente R2 de la función score:')\n",
    "print(lr.score(wisconsin_data,wisconsin_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Ejercicio MNIST </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# esta base de datos se puede descargar directamente de\n",
    "# https://www.openml.org/d/554\n",
    "# y le podemos indicar la ubicación en la que queremos que se almacene con data_home\n",
    "mnist = fetch_openml('mnist_784', version=1, data_home=\".\")\n",
    "\n",
    "# la base de datos descargada consiste en un diccionario con diferentes parámetros, de los\n",
    "# cuales sólo nos quedamos con data y target.\n",
    "\n",
    "# data consiste en una matriz donde cada fila es una imagen y las columnas representan los\n",
    "# pixels que la componen\n",
    "mnist_data = mnist['data']\n",
    "print(\"Número de imágenes: {0}\\tPixels por imagen: {1}\".format(mnist_data.shape[0],mnist_data.shape[1]))\n",
    "\n",
    "# target contiene las etiquetas de cada una de las imágenes, donde el valor de la clase \n",
    "# corresponde con el dígito que se muestra en la imagen\n",
    "mnist_target = mnist['target']\n",
    "print(\"Número de etiquetas: {0}\".format(mnist_target.shape[0]))\n",
    "\n",
    "# creamos un objeto con los parámetros por defecto\n",
    "#lr = LogisticRegression(solver='lbfgs', tol = 0.1, max_iter=5)\n",
    "lr = LogisticRegression(tol = 0.1)\n",
    "#lr = LogisticRegression()\n",
    "\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "lr.fit(mnist_data,mnist_target)\n",
    "\n",
    "# obtenemos una predicción para los datos de pima\n",
    "mnist_prediction = lr.predict(mnist_data)\n",
    "\n",
    "# comparamos estos datos con el error cuadrático medio\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('Error cuadrático medio:')\n",
    "print(mean_squared_error(mnist_target, mnist_prediction))\n",
    "\n",
    "# y obtenemos directamente el score\n",
    "print('Coeficiente R2 de la función score:')\n",
    "print(lr.score(mnist_data,mnist_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Matriz de confusion - Teoria</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "wisconsin = sklearn.datasets.load_breast_cancer()\n",
    "wisconsin.keys()\n",
    "wisconsin_data = wisconsin['data']\n",
    "wisconsin_target = wisconsin['target']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "X = wisconsin_data\n",
    "Y = wisconsin_target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "predicciones = lr.predict(X_test)\n",
    "print('Tasa de acierto o accuracy clasificador en el entorno de test: {:.2f}'.format(lr.score(X_test, Y_test)))\n",
    "\n",
    "# Matriz de confusion\n",
    "mc= confusion_matrix(y_true= Y_test, y_pred=predicciones)\n",
    "clases = np.unique(Y)\n",
    "print('Clase real\\n  {:}   {:}'.format(clases[0],clases[1]))\n",
    "print(mc)\n",
    "print('\\nTP={:d} TN={:d}'.format(mc[0,0], mc[1,1]))\n",
    "print('FP={:d}  FN={:d}'.format(mc[1,0], mc[0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio:  Análisis de datos en <i>Marketing</i></font> \n",
    "\n",
    "El objetivo final consiste en elaborar un modelo de detección del abandono. Estos pasos pueden servir de orientación en el desarrollo:\n",
    "\n",
    "- En este caso, se parte de un solo conjunto de datos, que se debe dividir en una partición de entrenamiento y otra de test, que se utilizará para validar.\n",
    "\n",
    "- Como paso previo, es puede ser interesante explorar la distribución de la clase. Para ello, puede hacerse, por ejemplo, un gráfico de barras.\n",
    "\n",
    "- Por último, habría que reportar el rendimiento (en acierto y coste) sobre el conjunto de test.\n",
    "\n",
    "Para la realización del ejercicio, se proporciona un conjunto de datos, denominado, `datos/churn/customerChurnAnalysis.csv`, relativo al abandono en una operadora de telecomunicaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "df = pd.read_csv('../1-ScikitLearn/datos/churn/customerChurnAnalysis.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation\n",
    "df.corr()\n",
    "#Visualize correlation between attributes by using heatmap\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature contains high correlation. We need to remove them first before applying regression techniques.\n",
    "# Create correlation matrix\n",
    "abs_corr_matrix = df.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select upper triangle of matrix\n",
    "up_tri = abs_corr_matrix.where(np.triu(np.ones(abs_corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find all the features which is having correlation > 0.75 with other features.\n",
    "correlated_features = [column for column in up_tri.columns if any(up_tri[column] > 0.75)]\n",
    "\n",
    "#Print correlated_features\n",
    "print(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pensar en hacer la correlación de columnas? \n",
    "#df.drop(['state','international plan','voice mail plan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "## Aqui una función para que cambie todos los object\n",
    "# Funcion no válida para boolean porque genera una matriz erronea\n",
    "## Este es el error que devuelve: LinAlgError: Singular matriz\n",
    "\n",
    "def encoder(df):\n",
    "    '''Manejo de campos categoricos para poder ser utilizados en scikit Learn'''\n",
    "    for col in df.columns:\n",
    "        if df.dtypes[col] == 'object':\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['churn'] = label_encoder.fit_transform(df['churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tras eliminar variables muy correlacionadas\n",
    "# Podemos hacer las particiones en arrays Numpy \n",
    "# Y = df.iloc[:,16].values\n",
    "# X = df.iloc[:,0:16].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Particiones train y test total day charge', 'total eve charge', 'total night charge', 'total intl charge\n",
    "y = df[\"churn\"]\n",
    "X = df[['state','account length','area code','phone number','international plan','number vmail messages','total day minutes',\n",
    "           'total day calls','total eve minutes','total eve calls','international plan','total night minutes',\n",
    "           'total night calls','total intl minutes','total intl calls','voice mail plan','customer service calls']]\n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Partición para train: \", X_train.shape, y_train.shape)\n",
    "print(\"Partición para test: \", X_test.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy del clasificador en el entorno de test: {:.2f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.estimators.plot_learning_curve(logistic_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit el modelo con la norma L1\n",
    "\n",
    "logistic_regression_l1 = LogisticRegression(penalty='l1',class_weight='balanced')\n",
    "logistic_regression_l1.fit(X_train, y_train)\n",
    "\n",
    "#Check on test data:\n",
    "\n",
    "y_pred = logistic_regression_l1.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#ROC Curve:\n",
    "\n",
    "y_pred_prob_l1 = logistic_regression_l1.predict_proba(X_test)\n",
    "class_1_prob = list()\n",
    "for i in y_pred_prob_l1:\n",
    "    class_1_prob.append(i[1])\n",
    "print(roc_auc_score(y_test, class_1_prob))\n",
    "\n",
    "model_result['Logistic Regression (L1)'] = roc_auc_score(y_test, class_1_prob)\n",
    "\n",
    "skplt.metrics.plot_roc_curve(y_test, y_pred_prob_l1, curves =['each_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test)\n",
    "\n",
    "#ROC curve:\n",
    "\n",
    "class_1_prob = list()\n",
    "for i in y_pred_prob:\n",
    "    class_1_prob.append(i[1])\n",
    "print(roc_auc_score(y_test, class_1_prob))\n",
    "\n",
    "model_result['Logistic Regression (L2)'] = roc_auc_score(y_test, class_1_prob)\n",
    "\n",
    "skplt.metrics.plot_roc_curve(y_test, y_pred_prob, curves=['each_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfica matriz de confusión con seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True,  fmt='');\n",
    "title = 'Matriz de Confusión sobre Regresión Logística con fit regularizado a 0.5'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfica matriz de confusión con scikit plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Churn sin columas categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../1-ScikitLearn/datos/churn/customerChurnAnalysisModified.csv')\n",
    "#df.to_csv(.\\datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"churn\"]\n",
    "X = df[['account length','area code','number vmail messages','total day minutes',\n",
    "           'total day calls','state','total eve minutes','total eve calls','international plan','total night minutes',\n",
    "           'total night calls','total intl minutes','total intl calls','voice mail plan','customer service calls']]\n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Partición para train: \", X_train.shape, y_train.shape)\n",
    "print(\"Partición para test: \", X_test.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy del clasificador en el entorno de test: {:.2f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True,  fmt='');\n",
    "title = 'Matriz de Confusión sobre Regresión Logística con fit regularizado a 0.5'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Titanic</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "#Hide all the warnings in jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we will build different model and compare them later. Let's store the result (AUC Score) in a dictionary.\n",
    "\n",
    "#Store result from different models\n",
    "model_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data in pandas dataframe\n",
    "titanic_data = pd.read_csv('../1-ScikitLearn/Titanic.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep selected columns\n",
    "titanic_data = titanic_data[['Survived','Pclass','Sex','Age','SibSp','Parch','Embarked','Fare']]\n",
    "titanic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Survived'].value_counts()\n",
    "sns.countplot(x='Survived',data=titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Age'] .fillna ((titanic_data['Age'] .mean()), inplace=True)\n",
    "titanic_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the data type of each feature:\n",
    "titanic_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_X = titanic_data[['Pclass','Sex','Age','SibSp','Parch', 'Embarked', 'Fare']]\n",
    "titanic_data_Y = titanic_data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(titanic_data_X, titanic_data_Y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Pclass',data=X_train)\n",
    "titanic_data_Y = titanic_data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of age data.\n",
    "sns.distplot(X_train['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_train['Fare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform Z-Score normalization on both these features. Age & Fare\n",
    "\n",
    "age_scaler = StandardScaler()\n",
    "age_scaler.fit(pd.DataFrame(X_train['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['Age']] = age_scaler.transform(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_scaler = StandardScaler()\n",
    "fare_scaler.fit(pd.DataFrame(X_train['Fare']))\n",
    "X_train[['Fare']] = fare_scaler.transform(X_train[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sex feature to 0,1 value.\n",
    "\n",
    "X_train['Sex'] = X_train['Sex'].map({'female': 0, 'male':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked has 3 categories. We can create three different variable which represents each category.\n",
    "embarked_encoder = preprocessing.LabelEncoder()\n",
    "embarked_encoder.fit(pd.DataFrame(X_train['Embarked']))\n",
    "X_train[['Embarked']] = embarked_encoder.transform(X_train[['Embarked']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlación entre variables\n",
    "sns.heatmap(X_train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare and Pclass has high correlation.\n",
    "# In logistic regression features should not be correlated. So, we can remove one variable.\n",
    "del X_train['Pclass']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "X_train_original = X_train\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "coefficients = pd.concat([pd.DataFrame(X_train_original.columns),pd.DataFrame(np.transpose(logistic_regression.coef_))],axis = 1)\n",
    "coefficients.columns = ['Feature','coefficient']\n",
    "coefficients = coefficients.append({'Feature':'Intercept','coefficient':logistic_regression.intercept_[0]}, ignore_index=True)\n",
    "coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model is created. We can test it on our test data. But first we need to transform the test data same as we did with training data.\n",
    "\n",
    "def transform_test_data(test_data,age_scaler,fare_scaler,embarked_encoder):\n",
    "    test_data['Sex'] = test_data['Sex'].map({'female': 0,'male': 1})\n",
    "    test_data[['Age']] = age_scaler.transform(test_data [['Age']])\n",
    "    test_data[['Fare']] = fare_scaler.transform(test_data [['Fare']])\n",
    "    test_data[['Embarked']] = embarked_encoder.transform(test_data[['Embarked']])\n",
    "    del test_data['Pclass']\n",
    "    return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_test_data(X_test,age_scaler,fare_scaler,embarked_encoder)\n",
    "\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will plot different classification result by using Scikit-plot library.\n",
    "# pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.estimators.plot_learning_curve(logistic_regression, X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_prob = logistic_regression.predict_proba(X_test)\n",
    "\n",
    "#ROC curve:\n",
    "\n",
    "class_1_prob = list()\n",
    "for i in Y_pred_prob:\n",
    "    class_1_prob.append(i[1])\n",
    "print(roc_auc_score(Y_test,class_1_prob))\n",
    "\n",
    "model_result['Logistic Regression (L2)'] = roc_auc_score(Y_test,class_1_prob)\n",
    "\n",
    "skplt.metrics.plot_roc_curve(Y_test, Y_pred_prob, curves=['each_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(Y_test, Y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with L1 norm:\n",
    "\n",
    "logistic_regression_l1 = LogisticRegression(penalty='l1',class_weight='balanced')\n",
    "logistic_regression_l1.fit(X_train,Y_train)\n",
    "\n",
    "#Check on test data:\n",
    "\n",
    "Y_pred = logistic_regression_l1.predict(X_test)\n",
    "print(accuracy_score(Y_test,Y_pred))\n",
    "\n",
    "#ROC Curve:\n",
    "\n",
    "Y_pred_prob = logistic_regression_l1.predict_proba(X_test)\n",
    "class_1_prob = list()\n",
    "for i in Y_pred_prob:\n",
    "    class_1_prob.append(i[1])\n",
    "print(roc_auc_score(Y_test,class_1_prob))\n",
    "\n",
    "model_result['Logistic Regression (L1)'] = roc_auc_score(Y_test,class_1_prob)\n",
    "\n",
    "skplt.metrics.plot_roc_curve(Y_test, Y_pred_prob,curves =['each_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
