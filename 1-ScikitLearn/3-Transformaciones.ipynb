{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/crowdlearning-etic.png\" alt=\"Logo ETIC\" align=\"right\">\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Transformaciones</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Fundamentos de Machine Learning</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#004D7F\">Transformaciones con SciKit Learn</font>\n",
    "\n",
    "Los transformadores son aquellos algoritmos y funciones que toman como datos de entrada las variables de nuestro problema y devuelven otras variables.\n",
    "Estas transformaciones suelen aplicarse antes de un algoritmo de clasificación/regresión, y todas aquellas operaciones que se ejecutan antes de un algoritmo de predicción reciben el nombre de preprocesamientos.\n",
    "\n",
    "En este apartado vamos a ver cómo funcionan las trasnformaciones en SciKit Learn. Para ello, vamos a aplicar distintas transformaciones a las bases de datos vistas. En SciKit Learn, todos los transformadores implementan dos funciones principales, `fit` y `transform`, y la que combina a ambos, `fit_transform`. \n",
    "\n",
    "## <font color=\"#004D7F\">Impute</font>\n",
    "\n",
    "Una de las trasnformaciones más __básicas__ consiste en la sustitución de valores perdidos o `NaN's` de la base de datos, ya que muchos algoritmos no son capaces de manejarlos. Una de las formas de realizar esta sustitución de valores perdidos consiste en utilizar la media (para valores continuos) o la moda (caso discreto) con los casos conocidos y aplicar dicha métrica a los `NaN's` de la variable.\n",
    "\n",
    "El módulo [__Impute__ en SciKit Learn](https://scikit-learn.org/stable/modules/impute.html) provee dos métodos para calcular y transformar:\n",
    "\n",
    "> La clase `sklearn.impute.SimpleImputer` se encarga de calcular y modificar los datos de entrada en una única dimensión de una variable.\n",
    "\n",
    "> `sklearn.impute.sklearn.IterativeImputer` utiliza el conjuto completo de características para estimar valores perdidos.\n",
    "\n",
    "\n",
    "Veamos cómo se utiliza en el caso de Wisconsin.\n",
    "\n",
    "### <font color=\"#004D7F\">Valores perdidos en Wisconsin</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wisconsin = pd.read_csv('wisconsin.csv')\n",
    "wisconsin.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo primero es separar los atributos de la clase.\n",
    "wisconsin_data = wisconsin.drop('label',1)\n",
    "wisconsin_target = wisconsin['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si existen valores perdidos `NaN's` Indicamos 3 métodos para mayor aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wisconsin_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wisconsin_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Después comprobamos los NaN's de nuestros datos, esto podemos hacerlo con Numpy.\n",
    "print(np.sum(np.isnan(wisconsin_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo la variable `bareNuclei` tiene valores perdidos, por lo que vamos a aplicar el `SimpleImputer` y ver que resultados se obtienen. Recordar que contiene tanto las funciones `fit` como `transform` y `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creamos un objeto de la clase Imputer con los parámetros por defecto\n",
    "imp = SimpleImputer()\n",
    "\n",
    "# Llamamos a fit, para que aprenda la media con la que tiene que rellenar los NaN's y le pasamos los datos\n",
    "imp.fit(wisconsin_data)\n",
    "\n",
    "# Finalmente, transformamos los datos con las medias aprendidas.\n",
    "wisconsin_trans = imp.transform(wisconsin_data)\n",
    "\n",
    "# Aunque le hayamos pasado un pandas a SciKit Learn, este nos devuelve un numpy array, por lo que volvemos a \n",
    "# trasnformarlo a pandas\n",
    "wisconsin_trans = pd.DataFrame(wisconsin_trans, columns = wisconsin_data.columns)\n",
    "\n",
    "# Si volvemos a comprobar los NaN's en la nueva base de datos, vemos que ya no hay\n",
    "print(np.sum(np.isnan(wisconsin_trans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto el método anterior como este son equivalentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(wisconsin_data)\n",
    "wisconsin_trans_1 = imp.transform(wisconsin_data)\n",
    "wisconsin_trans_1 = pd.DataFrame(wisconsin_trans, columns = wisconsin_data.columns)\n",
    "\n",
    "# Si volvemos a comprobar los NaN's en la nueva base de datos, vemos que ya no hay\n",
    "print(np.sum(np.isnan(wisconsin_trans_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible realizar el proceso en un sólo paso gracias a la función fit_transform y el resultado es el mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "wisconsin_fit_trans = imp.fit_transform(wisconsin_data)\n",
    "wisconsin_fit_trans = pd.DataFrame(wisconsin_fit_trans, columns = wisconsin_data.columns)\n",
    "\n",
    "# np.all nos devuelve true sólo si todas las comparaciones son ciertas, esto querrá decir que tanto el \n",
    "# fit y transform como el fit_transform han hecho lo mismo\n",
    "print(np.all(wisconsin_fit_trans == wisconsin_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, los valores perdidos han sido sustituidos por la media de los valores conocidos en `bareNuclei`. Si vamos a la documentación del `Imputer` en https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html, podremos observar los diferentes parámetros de configuración del algoritmo. Si queremos modificar el comportamiento del algoritmo deberemos definir estos parámetros\n",
    "en el constructor cuando creemos el objeto. \n",
    "\n",
    "#### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 1: Valores perdidos por la moda en Wisconsin</font>\n",
    "\n",
    "En este primer ejercicio se pide utilizar la mediana en vez de la media para rellenar los valores perdidos de `wisconsin_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\">Normalización por la media en MNIST</font>\n",
    "\n",
    "En teoría se ha visto cómo la normalización por la media ayuda a los algoritmos lineales a encontrar antes una solución. Este algoritmo también se utiliza en el procesamiento de imágenes y viene implementado en SciKit Learn con la clase `sklearn.preprocessing.StandardScaler`. Este transformador en su configuración por defecto, aplica la __normalización por la media__ de los atributos. \n",
    "\n",
    "Esto significa que pondera todas las variables por igual, esto significa que si disponemos de variables con 2 digitos, por ejemplo, edad y otra con 5 digitos, por ejemplo salario, se corre el peligro de que el modelo le otorgue mucha más importancia al salario que a la edad, y eso no es necesariamente exacto.\n",
    "\n",
    "#### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 2: Dígitos de MNIST normalizados por la media</font> \n",
    "\n",
    "Se pide de la misma forma que en el caso anterior, se pruebe este transformador y que se obtenga una nueva base de datos mnist_scaler y que se comparen los dígitos imprimidos antes con los transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, data_home=\".\")\n",
    "\n",
    "# la base de datos descargada consiste en un diccionario con diferentes parámetros, de los\n",
    "# cuales sólo nos quedamos con data y target.\n",
    "\n",
    "# data consiste en una matriz donde cada fila es una imagen y las columnas representan los\n",
    "# pixels que la componen\n",
    "mnist_data = mnist['data']\n",
    "print(\"Número de imágenes: {0}\\tPixels por imagen: {1}\".format(mnist_data.shape[0],mnist_data.shape[1]))\n",
    "\n",
    "# target contiene las etiquetas de cada una de las imágenes, donde el valor de la clase \n",
    "# corresponde con el dígito que se muestra en la imagen\n",
    "mnist_target = mnist['target']\n",
    "print(\"Número de etiquetas: {0}\".format(mnist_target.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un objeto StandardScaler y aplicar la función fit y transform a mnist_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ToDo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de MNIST classfification using multinomial logistic + L1\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Arthur Mensch <arthur.mensch@m4x.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 5000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50. / train_samples,\n",
    "                         multi_class='multinomial',\n",
    "                         penalty='l1', solver='saga', tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Class %i' % i)\n",
    "plt.suptitle('Classification vector for...')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Generación de nuevas variables</font>\n",
    "\n",
    "En este apartado vamos a hacer ciertas modificaciones usando la función `sklearn.preprocessing.PolynomialFeatures`. Este __transformador__ aumenta el número de variables aplicando funciones polinomiales en base al grado (parámetro de entrada) de la siguiente forma: \n",
    " * Si tenemos dos atributos [a, b], las nuevos atributos polinomiales de grado 2 serían [1, a, b, a^2, ab, b^2].\n",
    " \n",
    "#### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 3: Nuevas variables polinomiales de grado 2 en Pima</font> \n",
    "\n",
    "La forma de utilizar el algoritmo es igual que con `Imputer`, por eso se pide que utilizando `fit` y `transform` o `fit_transform`, se obtenga de `pima_data` una nueva base de datos llamada `pima_pol` aplicando `PolynomialFeatures`. Imprimir cuantas variables tiene ahora la base de datos y su nombre. Para obtener el nombre de las nuevas características mirar la función `get_feature_names` de la documentación en http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "Se pide: \n",
    "\n",
    "Aplicar este transformador en una nueva variable denominada `pima_pol` de igual manera que en el ejercio anterior.\n",
    "Se obtendran muchas variables nuevas a partir de las originales ¿nombre? ¿como se han generado? ¿como estan ordenadas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# ToDo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
