{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/crowdlearning-etic.png\" alt=\"Logo ETIC\" align=\"right\">\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Introducción a SciKit Learn</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Fundamentos de Machine Learning</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Bases de datos](#section2)\n",
    "* [3. Regresión/Clasificación con SciKit Learn](#section3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera práctica vamos a ver algunos conceptos básicos para trabajar con SciKit Learn (http://SciKit-learn.org/stable/ ). Se trata de una librería de Python que incluye una gran variedad de algoritmos de Machine Learning, además de otras herramientas que nos ayudarán tanto al prepocesamiento de los datos, como al testeo y validación de los modelos que obtengamos. Todas las prácticas de este curso y algunos apuntes de teoría utilizarán esta librería. \n",
    "También hay que destacar que SciKit Learn trabaja por defecto con las siguientes estructuras:\n",
    "* Array de numpy\n",
    "* DataFrame de pandas\n",
    "\n",
    "Es cierto que existen multitud de algoritmos y muchos de los que veremos en este curso ya se encuentran implementados. Sin embargo, es importante conocer la estructura que estos tienen dependiendo de si se trata de un clasificador, un preprocesamiento de datos, o una métrica. Esto es especialmente útil, ya que cuando nos enfrentemos a problemas reales relativos al Machine Learning.\n",
    "\n",
    "SciKit Learn dispone de un gran conjuto de datasets ( https://scikit-learn.org/stable/datasets/index.html ) En la siguiente sección de este notebook cargaremos algunos para su exploración.\n",
    "\n",
    "Para aquellos que deseen explorar, existe un gran conjunto de ejemplos relacionados con todos los algoritmos disponibles en SciKit Learn ya implementados ( https://scikit-learn.org/stable/auto_examples/ )\n",
    "\n",
    "Existen 4 interfaces principales que representan determinadas etapas del aprendizaje automático. Estas componentes no son excluyentes, por lo que los algoritmos pueden definir diferentes funciones en base a ellas:\n",
    "\n",
    "### <font color=\"#004D7F\">Estimador </font>\n",
    "Se trata de la base de SciKit Learn ya que define el método `fit`, encargado de aprender a partir de datos de entrada. \n",
    "La función `fit` en la que se basa el estimador tiene como parámetros de entrada los datos y, opcionalmente, un target de salida, que puede representar la __clase__ en un problema de clasificación o un __valor numérico__ en un problema de regresión:\n",
    "    \n",
    "    our_estimator.fit(data)\n",
    "\n",
    "ó\n",
    "\n",
    "    our_estimator.fit(data, targets)\n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\">Predictor </font>\n",
    "Para el aprendizaje supervisado y, en algunos casos, en problemas no supervisados queremos obtener un __output__ a partir de nuestros datos de entrada. En este caso necesitamos un predictor, en este caso la función que deberemos incluir en nuestra clase es `predict`. A partir de lo aprendido en la función `fit`, esta función tomará como entrada nuevos casos y obtendrá una predicción para ellos. \n",
    "\n",
    "    prediction = our_predictor.predict(data)\n",
    "    \n",
    "En clasificación es común que además de la clase se obtenga un valor de la certeza de esa clasificación, para ello existe la función opcional `predict_proba`, que nos permite obtener la certeza de un output con los mismos datos de entrada.\n",
    "\n",
    "    probability = our_predictor.predict_proba(data)\n",
    "\n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\">Transformador </font>\n",
    "Todos aquellos algoritmos dedicados a la transformación de los datos, como puede ser una eliminación de variables o el cálculo de valores perdidos, y que devuelven un conjunto modificado de los mismos son considerados transformadores. En este caso la función que hay que implementar es `transform`, que toma como parámetro los datos:\n",
    "    \n",
    "    new_data = our_transformer.transform(data)\n",
    "    \n",
    "Sin embargo, en el caso de las funciones de transformación __supervisadas__, hay que hacer un paso previo de entrenamiento con la función `fit`. En todos aquellos casos en los que realizar el entrenamiento y la transformación de los datos al mismo tiempo sea más eficiente se debe usar la función `fit_transform`:\n",
    "    \n",
    "    new_data = our_transformer.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\">Modelo </font>\t\n",
    "Aquellos algoritmos capaces de dar un valor de lo bien o mal que se ajusta su entrenamiento ante nuevos datos de entrada sobre los que se conoce su salida pueden implementar una función `score`. Valores como la __tasa de aciertos__ o la __matriz de confusión__ son algunos de los scores más utilizados para medir el rendimiento de los algoritmos.\n",
    "    \n",
    "    score = obj.score(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\"> 2. Bases de datos</font>\n",
    "\n",
    "Las bases de datos que es capaz de manejar SciKit Learn pueden tener diferentes formatos, lo más común es que utilicemos estructuras como el Array de numpy o el DataFrame de pandas. Estas bases de datos son de dos dimensiones, donde las filas representan cada uno de los casos o instancias y las columnas son los atributos o variables.\n",
    "\n",
    "Dependiendo del problema que estemos tratando, tendremos unas determinadas propiedades en la base de datos. Por ejemplo, en un problema de clasificación supervisado, dispondremos de una base de datos con casos donde cada uno tendrá datos de entrada (input) y una salida (output) que será discreta. En un problema de regresión, el caso es muy parecido, donde la salida es un valor continuo. Y en un problema de clasificación no supervisada, sólo tendremos datos de entrada.\n",
    "\n",
    "En este tutorial vamos a ver algunos ejemplos de bases de datos y con ellos vamos a utilizar algunas de los algoritmos ya implementados en SciKit Learn utilizando las funciones explicadas en la introducción. Estas bases de datos son pequeños ejemplos de algunos problemas reales, gracias a las funciones que vienen en SciKit Learn, es posible cargarlas sin mucho problema. Sin embargo, de la misma forma que usamos estas bases de datos podremos utilizar las nuestras propias que estén en pandas o numpy.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> \n",
    "Hay distintas formas de llamar a los \"atributos\" de un base de datos, como es __features__, datos de entrada, __input__, columnas, variables, etc. Por lo que si encontráis estos nombres, nos estamos refiriendo a lo mismo. Algo parecido ocurre cuando hablamos del \"valor a predecir\", que también se le conoce como __target__, output, salida, __label__, clase, etc.\n",
    "</div>\n",
    "\n",
    "<a id=\"section21\"></a> \n",
    "## <font color=\"#004D7F\">Base de datos Pima </font>\n",
    "\n",
    "Pima es una base de datos compuesta por casos de diabetes. Atributos como el peso o la presión sanguínea son los datos de entrada y la progresión de la diabetes a lo largo de un año es la salida. Se trata de un problema de __regresión__, ya que la salida son valores continuos. SciKit Learn incluye esta base de datos en sus funciones datasets. Ya que esta base de datos es un diccionario, podemos acceder a la descripción del problema con la clave `'DESCR'` y a los datos con `'data'` y `'target'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "pima = sklearn.datasets.load_diabetes()\n",
    "pima.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pima['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_data = pima['data']\n",
    "print(pima_data.shape)\n",
    "print(pima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_target = pima['target']\n",
    "print(pima_target.shape)\n",
    "print(pima_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pima_pandas = pd.DataFrame(pima['data'],columns = pima['feature_names'])\n",
    "pima_pandas['class'] = pd.Categorical(pima['target'])\n",
    "pima_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que esta base de datos viene en un array de numpy y que se ha modificado en un DataFrame de pandas, para que sea más entendible. Observamos que hay un total de 10 atributos de entrada (input) y 1 target (output), que se corresponden con las 11 columnas de nuestro dataset. También, que tenemos un total de 442 filas, que corresponde con el número de casos.\n",
    "\n",
    "Tanto el formato de array de numpy como el de DataFrame de pandas son aceptados por los algortimos de SciKit Learn, como veremos más adelante.\n",
    "\n",
    "<a id=\"section22\"></a> \n",
    "## <font color=\"#004D7F\">Base de datos Boston </font>\n",
    "Se trata de un problema de __regresión__, ya que la salida son valores continuos, es un dicionario que contiene gran cantidad de datos relacionados:\n",
    "\n",
    "* CRIM: crime rate per capita\n",
    "* ZN: proportion of residential land zoned\n",
    "* INDUS: proportion of non-retail business acres\n",
    "* CHAS: binary variable. 1 for tract bounds river and 0 otherwise\n",
    "* NOX: nitric oxides concentration\n",
    "* RM: average number of rooms\n",
    "* AGE: owner occupied units\n",
    "* DIS: weighted distance to employment centers.\n",
    "* RAD: index of accessibility\n",
    "* TAX: full value property tax rate\n",
    "* PTRATIO: pupil-teacher ratio\n",
    "* B: proportion of blacks\n",
    "* LSTAT: lower status of population\n",
    "* MEDV: owner occupied homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "boston = sklearn.datasets.load_boston()\n",
    "print(boston.data.shape)\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attributes or features of the data\n",
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.target[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = boston['data']\n",
    "boston_target = boston['target']\n",
    "#bostondf = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "#bostondf.head(4)\n",
    "#bostondf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section23\"></a> \n",
    "## <font color=\"#004D7F\">Base de datos Wisconsin </font>\n",
    "\n",
    "Wisconsin es una base de datos basada en un problema de __clasificación supervisada__. El objetivo es tratar de diagnosticar un bulto en el pecho como benigno o maligno a partir de 10 variables. \n",
    "\n",
    "SciKit Learn incluye esta base de datos en sus funciones datasets, cargamos la base de datos original de SciKit Learn con `sklearn.datasets.load_breast_cancer()`. Tiene una estructura de diccionario igual que en el caso anterior con Pima.\n",
    "\n",
    "A la base de datos original se le han aplicado una serie de transformaciones. Más adelante trabajaremos con otra vez de la original. Será cargada otra versión de un csv con pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn.datasets\n",
    "wisconsin = sklearn.datasets.load_breast_cancer()\n",
    "wisconsin.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wisconsin['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wisconsin_data = wisconsin['data']\n",
    "print(wisconsin_data.shape)\n",
    "print(wisconsin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wisconsin_target = wisconsin['target']\n",
    "print(wisconsin_target.shape)\n",
    "print(wisconsin_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# wisconsin = pd.read_csv('wisconsin.csv', dtype={ \"label\": 'category'})\n",
    "# wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes, volvemos a tener 10 atributos y una clase. En este caso hay 699 filas o casos.\n",
    "\n",
    "<a id=\"section24\"></a> \n",
    "## <font color=\"#004D7F\">Base de datos MNIST </font>\n",
    "\n",
    "Esta es la última base de datos. Es algo más compleja, pero su estructura es igual que las anteriores. Se trata de MNIST, un problema de clasificación supervisada con imágenes. Los datos de entrada son un conjunto de capturas (imágenes) de tamaño 28x28 pixels donde hay un dígito del 0 al 9 dibujado en cada una de ellas. Este dígito viene anotado con la etiqueta o clase correspondiente al número que se muestra en la imagen. La estructura es igual que en los casos anteriores, sólo que ahora tenemos los 784 pixels (28x28) que forman la imagen como atributos/columnas y 70000 imágenes.\n",
    "\n",
    "Para aquellos que no estéis familiarizados con la representación de imágenes, las de estas bases de datos se encuentran en escala de grises, lo que quiere decir que los únicos colores de las imágenes son el blanco, el negro y todos los grises que se encuentran en medio. Esta representación es más sencilla que una representación a color como puede ser RGB. Además, para reconocer dígitos sólo nos interesa su forma, no su color, por eso la escala de grises es más adecuada. Los valores de los atributos se encuentran en el intervalo [0,255] siendo el 0 la representación del negro y el 255 el blanco.\n",
    "\n",
    "Lo primero que vamos a hacer es descargar la base de datos y a previsualizar algunas de las imágenes para ver un poco más en detalle los diferentes dígitos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# esta base de datos se puede descargar directamente de\n",
    "# https://www.openml.org/d/554\n",
    "# y le podemos indicar la ubicación en la que queremos que se almacene con data_home\n",
    "mnist = fetch_openml('mnist_784', version=1, data_home=\".\")\n",
    "\n",
    "# la base de datos descargada consiste en un diccionario con diferentes parámetros, de los\n",
    "# cuales sólo nos quedamos con data y target.\n",
    "\n",
    "# data consiste en una matriz donde cada fila es una imagen y las columnas representan los\n",
    "# pixels que la componen\n",
    "mnist_data = mnist['data']\n",
    "print(\"Número de imágenes: {0}\\tPixels por imagen: {1}\".format(mnist_data.shape[0],mnist_data.shape[1]))\n",
    "\n",
    "# target contiene las etiquetas de cada una de las imágenes, donde el valor de la clase \n",
    "# corresponde con el dígito que se muestra en la imagen\n",
    "mnist_target = mnist['target']\n",
    "print(\"Número de etiquetas: {0}\".format(mnist_target.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vemos los datos, se trata de un numpy array de shape (70000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_data.shape)\n",
    "print(mnist_data)\n",
    "len(np.unique(mnist_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First row is first image\n",
    "first_image = mnist_data[0]\n",
    "first_label = mnist_target[0]\n",
    "\n",
    "# 784 columns correspond to 28x28 image\n",
    "plottable_image = np.reshape(first_image, (28, 28))\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(plottable_image, cmap='gray')\n",
    "plt.title('Digito: {}'.format(first_label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "images_to_plot = 9\n",
    "random_indices = random.sample(range(70000), images_to_plot)\n",
    "\n",
    "sample_images = mnist_data[random_indices, :]\n",
    "sample_labels = mnist_target[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "fig, axes = plt.subplots(3,3, \n",
    "                         figsize=(5,5),\n",
    "                         sharex=True, sharey=True,\n",
    "                         subplot_kw=dict(adjustable='box', aspect='equal'))\n",
    "for i in range(images_to_plot):\n",
    "    \n",
    "    # axes (subplot) objects are stored in 2d array, accessed with axes[row,col]\n",
    "    subplot_row = i//3 \n",
    "    subplot_col = i%3  \n",
    "    ax = axes[subplot_row, subplot_col]\n",
    "\n",
    "    # plot image on subplot\n",
    "    plottable_image = np.reshape(sample_images[i,:], (28,28))\n",
    "    ax.imshow(plottable_image, cmap='gray_r') # Si se quiere usar invertido = 'gray'\n",
    "    \n",
    "    ax.set_title('Digito: {}'.format(sample_labels[i]))\n",
    "    ax.set_xbound([0,28])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\"> 3. Regresión/Clasificación con SciKit Learn</font>\n",
    "\n",
    "De momento, cuando hablamos de clasificación y regresión nos referimos a problemas de __predicción supervisada__, donde tenemos casos en los que se conoce la salida que queremos obtener. En todas las bases de datos que hemos visto, hay una columna que es el `target` o la clase.\n",
    "\n",
    "En SciKit Learn, muchos de los algoritmos más utilizados ya vienen implementados, cómo la __regresión lineal__ y la __regresión logística__. Todos los algoritmos de predicción implementan dos funciones: `fit` y `predict`. En algunos casos, es posible obtener una __métrica__ de lo buena o mala que es la predicción utilizando la función `score`. Esta función, que ya se ha explicado en la introducción, recibe como datos de entrada los atributos y los target. Sobre el primero realiza una predicción y luego compara con el target, para obtener una métrica cómo la tasa de aciertos en los problemas de clasificación o el error cuadrático medio en regresión.\n",
    "\n",
    "<a id=\"section31\"></a> \n",
    "## <font color=\"#004D7F\">Regresión lineal en Pima</font>\n",
    "\n",
    "La base de datos Pima es un problema de regresión, ya que el `target` o clase tiene __valores continuos__, por lo que hay que utilizar un algoritmo de regresión.\n",
    "\n",
    "Para ello, vamos a ver el primer estimador de SciKit Learn, el __regresor lineal__, que viene implementado en `sklearn.linear_model.LinearRegression`. Vamos a utilizar las funciones `fit`, `predict` y `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# creamos un objeto con los parámetros por defecto\n",
    "lr = LinearRegression()\n",
    "\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "lr.fit(pima_data,pima_target)\n",
    "\n",
    "# obtenemos una predicción para los datos de pima\n",
    "pima_prediction = lr.predict(pima_data)\n",
    "\n",
    "# comparamos estos datos con el error cuadrático medio\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('Error cuadrático medio:')\n",
    "print(mean_squared_error(pima_target, pima_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto, la función score devuelve el coeficiente de determinación R2\n",
    "# no es objetivo de esta práctica explicarlo, sólo sirve de ejemplo para ver cómo se usa la función score\n",
    "\n",
    "# Volvemos a crear un objeto con los parámetros por defecto\n",
    "lr = LinearRegression()\n",
    "\n",
    "# entrenamos con los datos de entrada y la salida\n",
    "lr.fit(pima_data,pima_target)\n",
    "\n",
    "# y obtenemos directamente el score\n",
    "print('Coeficiente R2 de la función score:')\n",
    "print(lr.score(pima_data,pima_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section32\"></a> \n",
    "## <font color=\"#004D7F\">Regresión lineal en Boston</font>\n",
    "\n",
    "En este caso, tenemos un problema de clasificación supervisada. Para resolver este problema vamos a cambiar al regresor logístico y cómo vamos a ver, el problema se resuelve de forma identica al caso anterior, por lo que esta parte le toca al alumno completarlo. \n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 1: Regresión logística para Boston</font> \n",
    "\n",
    "Para este ejercicio, importamos de `sklearn.linear_model.LinearRegression` el algoritmo y utilizando `fit` y `predict`. Y utilizando `boston_data` y `boston_target` hay que calcular la tasa de aciertos y compararla con la salida de `score`, que en este caso, también se trata de la tasa de aciertos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section33\"></a> \n",
    "## <font color=\"#004D7F\">Regresión logística en MNIST</font>\n",
    "\n",
    "Aunque se traten de imágenes, la estructura del problema es el de uno de clasificación supervisada. Vamos a volver a utilizar el regresor logístico y cómo vamos a ver, el problema se resuelve de forma identica al caso anterior, por lo que otra vez será el alumno el que lo complete. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "_Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function._\n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 2: Regresión logística para MNIST</font> \n",
    "\n",
    "Para este ejercicio, importamos de `sklearn.linear_model.LogisticRegression` el algoritmo y utilizando `fit` y `predict`. Y utilizando `mnist_data` y `mnist_target` hay que calcular la tasa de aciertos y compararla con la salida de `score`, que en este caso, también se trata de la tasa de aciertos.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "__Importante__: Al ser un problema mucho mayor, deberemos modificar algunos parámetros de entrada del clasificador. Para ello recomendamos mirar la documentación de http://SciKit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html. Como pista, para que el algoritmo se ejecute en un tiempo razonable, con multiplicar por 100 el campo `tol` es más que suficiente.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section34\"></a> \n",
    "## <font color=\"#004D7F\">Regresión logística en Wisconsin</font>\n",
    "\n",
    "En este caso, tenemos un problema de clasificación supervisada. Para resolver este problema vamos a cambiar al regresor logístico y cómo vamos a ver, el problema se resuelve de forma identica al caso anterior, por lo que esta parte le toca al alumno completarlo. \n",
    "\n",
    "\n",
    "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 3: Regresión logística para Wisconsin</font> \n",
    "\n",
    "Para este ejercicio, importamos de `sklearn.linear_model.LogisticRegression` el algoritmo y utilizando `fit` y `predict`. Y utilizando `wisconsin_data` y `wisconsin_target` hay que calcular la tasa de aciertos y compararla con la salida de `score`, que en este caso, también se trata de la tasa de aciertos.\n",
    "\n",
    "Con esto, ya hemos visto algunos tipos de predictores de SciKit Learn y que independientemente de si es un problema de clasificación o regresión, las funciones son las mismas. Además, hemos visto como pueden utilizarse tanto datos de numpy o pandas. Aunque hay que tener en cuenta que tanto las transformaciones como las predicciones de SciKit Learn van a estar en un array de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No vamos a detenernos en lo bueno o malo que son los clasificadores, ya que tendremos una práctica en la que analizaremos diferentes clasificadores y trataremos de mejorar los resultados. Estos ejemplos sirven para ver cómo utilizar SciKit Learn.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
