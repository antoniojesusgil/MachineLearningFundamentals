{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/crowdlearning-etic.png\" alt=\"Logo ETIC\" align=\"right\">\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Selección de Variables</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Fundamentos de Machine Learning</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Introducción a métodos de selección de variables</font>\n",
    "\n",
    "El objetivo de la regresión __lasso__ es obtener el subconjunto de predictores que minimiza el error de predicción para una variable de respuesta cuantitativa. Lasso hace esto al imponer una restricción en los parámetros del modelo que hace que los coeficientes de regresión para algunas variables se reduzcan a cero. Las variables con un coeficiente de regresión igual a cero después del proceso de contracción se excluyen del modelo. \n",
    "\n",
    "Las variables con coeficientes de regresión distintos de cero están más fuertemente asociadas con la variable _target_. Por lo tanto, cuando se realiza un modelo de regresión, puede ser útil hacer una regresión __lasso__ para predecir cuántas variables debe contener el modelo. Esto asegura que su modelo no sea demasiado complejo y evita que el modelo se ajuste demasiado, lo que puede resultar en un modelo sesgado e ineficiente.\n",
    "\n",
    "\n",
    "En general, existen métodos para la contracción y la selección variables de un modelo de regresión lineal. Estas son regresión de __lasso__ y regresión __ridge__. \n",
    "\n",
    "La regresión __ridge__ es básicamente un modelo de regresión lineal regularizado. El parámetro `λ` es un escalar que también se debe aprender, utilizando un método llamado validación cruzada.\n",
    "\n",
    "La única diferencia entre estos dos métodos es que el término de regularización se obtiene un valor absoluto. Pero esta diferencia tiene un gran impacto en la compensación. \n",
    "\n",
    "El método __lasso__ supera la desventaja de la regresión __ridge__ al no solo castigar los valores altos de los coeficientes `β`, sino que también los establece en cero si no son relevantes. Por lo tanto, puede terminar con menos funciones incluidas en el modelo de las que comenzó, lo cual es una gran ventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\">Veamos un ejemplo</font>\n",
    "\n",
    "Sobre los precios de venta en viviendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from scipy.stats import skew\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats.stats import pearsonr\n",
    "%matplotlib inline\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./train.csv')\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "train.shape\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigar el dataset\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "all_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesado del dataset \n",
    "# Data PreProcessing\n",
    "new_price = {\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])}\n",
    "prices = pd.DataFrame(new_price)\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 5.0)\n",
    "prices.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "skewed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "all_data = pd.get_dummies(all_data)\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#004D7F\">Modelling a ridge regression in Python</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model Ridge regression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(Ridge(), X_train, y, scoring=\"neg_mean_squared_error\", cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_ridge = np.sqrt(-cross_val_score(Ridge(), X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "rmse_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at a chart about the coefficients in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "            for alpha in alphas]\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ridge.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#004D7F\">Modelling a lasso regression in Python</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model Lasso regression\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
    "rmse_cv(model_lasso).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Model Lasso regression\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual model\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "preds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n",
    "preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n",
    "preds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#004D7F\">Tests ANOVA y Mutual Information</font> \n",
    "\n",
    "Son métricas univariadas para problemas de clasificación\n",
    "\n",
    "> Test paramétrico __ANOVA__, tiene como hipótesis nula el que, para cada posible valor de la clase, la media de una variable no varía. Solo captura relaciones lineales\n",
    "\n",
    "> __MI__, Mutual Information, captura cualquier tipo de relación\n",
    "\n",
    "Cuanto mayor es el valor de la métrica, más relacion del atributo con la clase. Un valor 0 significa independencia total entre variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\"> Ranking por test ANOVA</font> \n",
    "\n",
    "Selecciona las K mejores variables con __SelectKBest__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "datos = sklearn.datasets.load_breast_cancer()\n",
    "X = datos.data\n",
    "Y = datos.target\n",
    "# Añadimos 100 variables con ruido despues de las 30 originales \n",
    "# No deberian ser seleccionadas\n",
    "semilla = 0\n",
    "random = np.random.RandomState(semilla)\n",
    "ruido = random.normal(size=(len(X), 100))\n",
    "X_ruido = np.hstack([X, ruido])\n",
    "X_t, X_T, Y_t, Y_T = train_test_split(X_ruido,Y,test_size=0.2, random_state=semilla)\n",
    "\n",
    "# Seleccionar las 10 mejores segun test ANOVA\n",
    "K = 10\n",
    "ranking = SelectKBest(score_func = f_classif, k=K)\n",
    "ranking.fit(X_t,Y_t)\n",
    "X_t_proyec = ranking.transform(X_t)\n",
    "\n",
    "# Ver ranking visual y de variables seleccionadas \n",
    "print(\"Ranking: {}\".format(np.argsort(ranking.scores_)[::-1][:K]))\n",
    "seleccionados = ranking.get_support()\n",
    "plt.matshow(seleccionados.reshape(1,-1), cmap='prism')\n",
    "plt.xlabel(\"Indice de la variable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#004D7F\"> Ranking por test ANOVA</font> \n",
    "\n",
    "Porcentaje de variables seleccionadas utilizando __SelectPercentile__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seleccion = SelectPercentile(f_classif)\n",
    "pipe = Pipeline([('seleccion', seleccion), ('lr', LogisticRegression())])\n",
    "\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "porcentajes = [1,5] + list(np.linspace(10,100,10))\n",
    "\n",
    "for p in porcentajes:\n",
    "    pipe.set_params(seleccion__percentile=p)\n",
    "    this_scores = cross_val_score(pipe, X, Y)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "    \n",
    "plt.errorbar(porcentajes, score_means, np.array(score_stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font color=\"#004D7F\"> Evaluación de métodos de selección</font> \n",
    "\n",
    "Este ejemplo obtiene una evaluación de la Regresión logística utilizando tanto el método __ANOVA__ como __MI__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# Seleccionar las 10 mejores segun test ANOVA\n",
    "K = 10\n",
    "ranking_F = SelectKBest(score_func = f_classif, k=K)\n",
    "ranking_F.fit(X_t,Y_t)\n",
    "X_seleccionF = ranking_F.transform(X_t)\n",
    "\n",
    "# Seleccionar las 10 mejores por MI\n",
    "K = 10\n",
    "ranking_MI = SelectKBest(score_func = mutual_info_classif, k=K)\n",
    "ranking_MI.fit(X_t,Y_t)\n",
    "X_seleccionMI = ranking_MI.transform(X_t) # proyectar\n",
    "\n",
    "# crear modelos\n",
    "lr = LogisticRegression(); lr_F = LogisticRegression(); lr_MI = LogisticRegression();\n",
    "lr.fit(X_t,Y_t); lr_F.fit(X_seleccionF,Y_t); lr_MI.fit(X_seleccionMI, Y_t)\n",
    "\n",
    "# Proyectar test y validar modelos\n",
    "XT_F =  ranking_F.transform(X_T); XT_MI = ranking_MI.transform(X_T)\n",
    "score = lr.score(X_T, Y_T)\n",
    "scoreF = lr_F.score(XT_F, Y_T)\n",
    "scoreMI = lr_MI.score(XT_MI, Y_T)\n",
    "\n",
    "print('Acc sin seleccion: {:.4f}'.format(score))\n",
    "print('Acc con 10 mejores ANOVA: {:.4f}'.format(scoreF))\n",
    "print('Acc con 10 mejores MI: {:.4f}'.format(scoreMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
