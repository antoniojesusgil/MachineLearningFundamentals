{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/crowdlearning-etic.png\" alt=\"Logo ETIC\" align=\"right\">\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Naïve Bayes</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Fundamentos de Machine Learning</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto más importante de este algoritmo radica en que no asume relación alguna entre las features del dataset.\n",
    "\n",
    "El algoritmo Naïve Bayes es utilizado para problemas de clasificación. Es extremadamente utilizado en clasificación textual.\n",
    "\n",
    "En las tareas de clasificación de texto, los datos contienen una dimensión alta (ya que cada palabra representa una característica en los datos). Se utiliza en el filtrado de spam, detección de sentimientos, rating de clasificación, etc. La ventaja de usar naïve Bayes es su velocidad. Es rápido y hacer predicciones es fácil con una gran dimensión de datos.\n",
    "\n",
    "Este modelo predice la probabilidad de que una instancia pertenezca a una clase con un conjunto de características dado. Es un clasificador probabilístico. Se llama naïve (ingenuo) porque supone que una característica en el modelo es independiente de la existencia de otra característica. En otras palabras, cada característica contribuye a las predicciones sin relación entre sí. En el mundo real, esta condición rara vez se satisface. Utiliza el teorema de Bayes en el algoritmo para entrenamiento y predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/naiveBayes.png\" height=\"250\" width=\"250\">\n",
    "\n",
    "- P(X|Y) es la probabilidad del evento X dado que el evento Y es verdadero. (Probabilidad posterior)\n",
    "- P(Y|X) es la probabilidad del evento Y dado que el evento X es verdadero. (Probabilidad)\n",
    "- P(X) probabilidad del evento X. (Prior Probability)\n",
    "- P(Y) probabilidad del evento Y. (Evidence)\n",
    "\n",
    "En otras palabras, podemos afirmar que la probabilidad de X dado Y es igual a la probabilidad de Y dado X multiplicado por la probabilidad de X dividido por la probabilidad de Y.\n",
    "\n",
    "<img src=\"../img/probabilidad.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "Disponemos de un mazo con 52 cartas. La primera tarea es calcular la probabilidad de que la carta contega un rey (king).\n",
    "\n",
    "- Total cartas = 52\n",
    "- Total cartas King = 4\n",
    "- Probabilidad de que una carta sea un king -> P(King) = 4/52 = 1/13\n",
    "\n",
    "Ahora, calcularemos la probabilidad de una carta sea un king si esa carta contiene una cara (face)\n",
    "\n",
    "> `P(King/Face) = P(King/Face)*P(King) / P(Face)`\n",
    "    \n",
    "- P(King) = 1/13\n",
    "- P(Face) = 9/52 = 3/13\n",
    "\n",
    "Aplicando la fórmula anterior: \n",
    "\n",
    "> `P(King/Face) = ((1/13) * 1) / (3/13) = 1/3`\n",
    "\n",
    "Consideraremos el problema de clasificación de aprendizaje automático. Tenemos `k` características con clases `C`. El objetivo es predecir la probabilidad de que una __instancia__ pertenezca a una clase `Ci`. El algoritmo Naïve Bayes toma el vector de características y calcula la probabilidad condicional de la clase `Ci` con este conjunto de características, esto mismo se realiza para todas las clases y la clase que tiene la __probabilidad máxima__ se informa como __clase predicha__.\n",
    "\n",
    "Las ventajas de Naïve Bayes son:\n",
    "\n",
    "- Es fácil y rápido de implementar\n",
    "- Se desempeña bien en presencia de características categóricas. \n",
    "- Para las características numéricas, se supone que los datos provienen de distribuciones normales.\n",
    "\n",
    "Si la característica categórica contiene alguna nueva categoría en los datos de prueba, es asignada como probabilidad cero, en ese caso se aplica la transformación de __Laplace__ para manejar este problema.\n",
    "\n",
    "Además, la suposición de predicción independiente rara vez está presente en el mundo real.\n",
    "\n",
    "## <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Titanic</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "#Hide all the warnings in jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we will build different model and compare them later. Let's store the result (AUC Score) in a dictionary.\n",
    "\n",
    "#Store result from different models\n",
    "model_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load data in pandas dataframe\n",
    "titanic_data = pd.read_csv('../1-ScikitLearn/Titanic.csv', sep='\\t')\n",
    "\n",
    "#Keep selected columns\n",
    "titanic_data = titanic_data[['Survived','Pclass','Sex','Age','SibSp','Parch','Embarked','Fare']]\n",
    "titanic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Survived'].value_counts()\n",
    "sns.countplot(x='Survived',data=titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data['Age'] .fillna ((titanic_data['Age'] .mean()), inplace=True)\n",
    "titanic_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the data type of each feature:\n",
    "titanic_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_X = titanic_data[['Pclass','Sex','Age','SibSp','Parch', 'Embarked', 'Fare']]\n",
    "titanic_data_Y = titanic_data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(titanic_data_X, titanic_data_Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto se ha de tener en cuenta que se realizan las particiones para `train`y `test` y son pasadas por `standardScaler` cuya idea principal es normalizar/estandarizar (mean = 0 y standard deviation = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Pclass',data=X_train)\n",
    "titanic_data_Y = titanic_data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of age data.\n",
    "sns.distplot(X_train['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_train['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform Z-Score normalization on both these features. Age & Fare\n",
    "\n",
    "age_scaler = StandardScaler()\n",
    "age_scaler.fit(pd.DataFrame(X_train['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['Age']] = age_scaler.transform(X_train[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_scaler = StandardScaler()\n",
    "fare_scaler.fit(pd.DataFrame(X_train['Fare']))\n",
    "X_train[['Fare']] = fare_scaler.transform(X_train[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sex feature to 0,1 value.\n",
    "\n",
    "X_train['Sex'] = X_train['Sex'].map({'female': 0, 'male':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked has 3 categories. We can create three different variable which represents each category.\n",
    "embarked_encoder = preprocessing.LabelEncoder()\n",
    "embarked_encoder.fit(pd.DataFrame(X_train['Embarked']))\n",
    "X_train[['Embarked']] = embarked_encoder.transform(X_train[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlación entre variables\n",
    "sns.heatmap(X_train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare and Pclass has high correlation.\n",
    "# In logistic regression features should not be correlated. So, we can remove one variable.\n",
    "del X_train['Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos aplicado `standardScaler` tan solo al conjunto de entrenamiento, ahora hay que hacer lo mismo para el de test. En este caso se ha creado una función que realiza todo el trabajo anterior en un único paso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_data(test_data,age_scaler,fare_scaler,embarked_encoder):\n",
    "    test_data['Sex'] = test_data['Sex'].map({'female': 0,'male': 1})\n",
    "    test_data[['Age']] = age_scaler.transform(test_data [['Age']])\n",
    "    test_data[['Fare']] = fare_scaler.transform(test_data [['Fare']])\n",
    "    test_data[['Embarked']] = embarked_encoder.transform(test_data[['Embarked']])\n",
    "    del test_data['Pclass']\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_test_data(X_test,age_scaler,fare_scaler,embarked_encoder)\n",
    "\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train,Y_train)\n",
    "\n",
    "# Check on test data:\n",
    "\n",
    "Y_pred = naive_bayes.predict(X_test)\n",
    "Y_pred_prob = naive_bayes.predict_proba(X_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva de aprendizaje:\n",
    "\n",
    "skplt.estimators.plot_learning_curve(naive_bayes,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "skplt.metrics.plot_confusion_matrix(Y_test, Y_pred,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve:\n",
    "\n",
    "Y_pred_prob = naive_bayes.predict_proba(X_test)\n",
    "class_1_prob = list()\n",
    "for i in Y_pred_prob:\n",
    "    class_1_prob.append(i[1])\n",
    "    \n",
    "print(roc_auc_score(Y_test,class_1_prob))\n",
    "\n",
    "model_result['Naive Bayes'] = roc_auc_score(Y_test,class_1_prob)\n",
    "\n",
    "skplt.metrics.plot_roc_curve(Y_test, Y_pred_prob,curves=['each_class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
