{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/crowdlearning-etic.png\" alt=\"Logo ETIC\" align=\"right\">\n",
    "\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=6>Evaluación y validación de modelos en SciKit Learn</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Antonio Jesús Gil</font><br>\n",
    "<font color=\"#004D7F\" size=3>Fundamentos de Machine Learning</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\"> 1. Introducción</font>\n",
    "\n",
    "A partir de los ejemplos realizados con regresión lineal y logística, hemos visto que para crear un modelo es necesario utilizar una base de datos de la que extraemos un array bidimensional $X$ siendo $X.shape=(m,n)$ y que contiene para cada registro el valor de $n$ variables descriptivas; y un array $Y.shape=(m,)$ que contiene el valor de la variable clase para cada uno de los _m_ registros.\n",
    "\n",
    "Al utilizar $X$ e $Y$ para aprender el array en un modelo de regresión lineal, se dice que $(X,Y)$ es el __conjunto de entrenamiento__: una serie de registros descritos por las variables en $X$, y para cada uno de estos registros conocemos el valor de la variable clase  gracias a que disponemos de $Y$.\n",
    "\n",
    "Existen más modelos además de la regresión. Estos _modelos_, _clasificadores_ o _estimadores_ necesitan un conjunto de entrenamiento para aprender parámetros, reglas, proyección en el espacio de coordendas, etc., según el tipo de clasificador que se esté utilizando. A este proceso de aprendizaje se le conoce también como __estimación__, __construcción__, __entrenamiento__ o __ajuste__ del modelo.\n",
    "\n",
    "- `.fit` Los modelos disponbiles en _SciKit Learn-learn_ son aprendidos a partir de la función disponible en todos ellos $.fit(X,Y)$.\n",
    "\n",
    "Una vez construido un clasificador, queremos saber cómo de bueno es; es decir, evaluarlo. La __evaluación__ de un clasificador, una vez ya ha sido entrenado, consiste en comprobar cuál es su rendimiento al predecir la variable clase en datos nuevos no utilizados en el aprendizaje del model. Por supuesto los datos nuevos deben contener el mismo tipo y formato de las variables predictivas. Así, para evaluar un modelo dispondremos, en el caso más simple, de un conjunto de entrenamiento $(X_t,Y_t)$ y un conjunto de test $(X_T,Y_T)$. A partir de $X_T$ se estimará $\\hat{Y}_T$ para después compararlo con $Y_T$\n",
    "\n",
    "- `.score` Los modelos disponibles en _SciKit Learn-learn_ calculan su rendimiento sobre $(X_T,Y_T)$ a partir de la función disponible $.score(X_T,Y_T)$. La métrica que esta función devuelve es la tasa de aciertos (_accuracy_), con rango [0-1]\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>\n",
    "__OJO!__ Por muy tentador que sea, el conjunto de test nunca debe ser utilizado para entrenar nuestro modelo. Aunque conozcamos la variable clase $Y_T$, ésta solo debe ser utilizada para compararla con $\\hat{Y}_T$ y así ver cómo de bueno es el clasificador construido.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3><font color=\"#004D7F\" size=4> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> Ejercicio 1: Creación de $(X,Y)$, $(X_t,Y_t)$, $(X_T,Y_T)$ y validación train/test o holdout.</font></h3>\n",
    "\n",
    "\n",
    "Cargamos la base de datos `Breast Cancer Wisconsin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "wisconsin = sklearn.datasets.load_breast_cancer()\n",
    "wisconsin.keys()\n",
    "wisconsin_data = wisconsin['data']\n",
    "wisconsin_target = wisconsin['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wisconsin_data.shape, wisconsin_data.dtype)\n",
    "print(wisconsin_target.shape, wisconsin_target.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tasa de aciertos sería buena para esta base de datos? Para tener en mente una línea base, vamos a mirar la distribución de la clase. Después, calcula cuál sería la tasa de aciertos de un clasificador que siempre predice la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(wisconsin_target)\n",
    "print(counts)\n",
    "acc_baseline=counts[0]*100 / wisconsin_target.shape[0]\n",
    "print('Un clasificador debería obtener al menos una tasa de aciertos = {:.2f}%'.format(acc_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Seleccionar modelo de regresión apropiado. ¿LinearRegression o LogisticRegression?\n",
    "lr = ???\n",
    "\n",
    "# Entrenar\n",
    "lr. ???\n",
    "\n",
    "# Evaluar el modelo con la misma base de datos con la que se ha entrenado\n",
    "acc = lr.score(wisconsin_data,wisconsin_target)\n",
    "print('{:.4f}'.format(acc))\n",
    "\n",
    "# 0.9596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado un modelo predictivo de cáncer de pecho con una tasa de aciertos = $95.96\\%$, mucho mayor que nuestra línea base. Pero es que... __¡hemos hecho trampa!__ ¿Por qué?\n",
    "\n",
    "Ahora crearemos un partición de la base de datos $(X,Y)$ de manera que obtengamos un conjunto de entrenamiento $(X_t,Y_t)$  con el 70% de los casos, y con el resto se creará el conjunto de test $(X_T,Y_T)$. Después, entrena y evalúa tu modelo de manera correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_Test,Y_train,Y_Test = train_test_split(wisconsin_data,wisconsin_target,test_size=0.3, random_state=10)\n",
    "# random_state, semilla para partición aleatoria\n",
    "\n",
    "lr.???\n",
    "\n",
    "acc = lr.???\n",
    "\n",
    "print('{:.4f}'.format(acc))\n",
    "\n",
    "# 0.9474"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> La validación <b>holdout</b> o división <b>train/test</b> consite en dividir el conjunto de datos disponible $(X,Y)$ en un conjunto de entrenamiento $(X_t, Y_t)$ y otro de test $(X_T,Y_T)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar si podemos fiarnos de la validación holdout, validaremos con 10 semillas diferentes a la hora de realizar la partición train/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lr = LogisticRegression()\n",
    "K = 10\n",
    "acc = np.empty(K)\n",
    "\n",
    "for i in range(0,K):\n",
    "    # usa como semilla de la partición la propia iteración i\n",
    "    X_t, X_T, Y_t, Y_T = train_test_split(wisconsin_data, wisconsin_target, test_size=0.3, random_state=i*i) \n",
    "    lr.fit(X_t,Y_t)\n",
    "    acc[i] = lr.score(X_T,Y_T)\n",
    "    print('{:.4f}'.format(acc[i]))\n",
    "print('\\nAcc:{:.4f} Std:{:.4f}'.format(np.mean(acc),np.std(acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> Se dice que un modelo <b>generaliza</b> bien cuando su rendimiento no empeora (mucho) al ser evaluado sobre el conjunto de test.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <font color=\"#004D7F\"> 2. Validación Cruzada</font>\n",
    "\n",
    "Cuando se dispone de un conjunto muy grande de datos, la validación _holdout_ se considera apropiada para estimar el poder de generalización de un modelo; es decir, a __más datos disponibles tendremos una menor varianza__ del rendimiento del clasificador. Sin embargo, dicha base de datos debe ser especialmente larga (dependiendo del clasificador utilizado) para asegurarnos de que el rendimiento obtenido no es dependiente de la partición concreta que hemos hecho.\n",
    "\n",
    "Hay 3 decisiones que debemos tomar a la hora de crear nuestros conjuntos de entrenamiento y test. Las 3 dependen de las características de nuestra base de datos.\n",
    "\n",
    "- _Barajar_: es posible que los datos cargados estén ordenados por alguna variable concreta, y que ésta tenga relación con la variable clase a predecir. Por ello, al realizar particiones puede ser que aprendamos modelos muy diferentes según el punto de corte entre un conjunto de entrenamiento y el de test. Es por eso que siempre es recomendable barajar los registros de la base de datos antes de particionar. En el caso concreto de la función sklearn.model_selection.train_test_split, esto se indica mediante el parámetro `shuffle` (por defecto `shuffle=True`).\n",
    "\n",
    "\n",
    "- _Estratificar_: Tanto antes como después de barajar nuestra base de datos, al extraer los conjuntos de entrenamiento y test es posible que la distribución de la variable clase sea muy diferente entre ($X_t,Y_t$) y ($X_T,Y_T$), lo cual haría imposible que nuestro modelo generalice bien. Además, crear particiones estratificadas parece lo más natural ya que se espera que lo que ocurre en un período de tiempo (entrenamiento) ocurra también en el futuro (test); aunque por supuesto esto no es siempre así cuando la clase tiene una dependencia temporal. En SciKit Learn-learn se puede utilizar el parámetro `stratify`en la función sklearn.model_selection.train_test_split, o directamente usar la función `StratifiedShuffleSplit.split` (devuelve los índices seleccionados de $X$ para crear los conjuntos de train y test).\n",
    "\n",
    "\n",
    "- _Validación cruzada_: Si se dispone de pocos datos obtendremos un rendimiento del modelo que dependerá mucho de la semilla y tamaño seleccionados para crear los conjuntos de entrenamiento y test. También, no podemos saber a priori si tenemos datos suficientes para calcular el poder real de generalización de nuestro modelo (es posible que sea mejor cuantos más datos tengamos). Por ello, es muy frecuente utilizar la K-validación cruzada o __K-CV__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> La <b>validación cruzada</b> o <b>K-CV</b> consiste en dividir (muestrear sin reemplazo) el conjunto de datos $(X,Y)$ de manera que tengamos K conjuntos disjuntos de test ($X_T^i,Y_T^i$), para $i={1,...,K}$. Dado un conjunto de test, el conjunto de entrenamiento será el resto de datos.\n",
    "<br>\n",
    "Es decir, $(X_t^i,Y_t^i) = (X,Y) - (X_T^i,Y_T^i)$. El rendimiento estimado del modelo será la media de las K validaciones realizadas, cada una utilizando el conjunto de entrenamiento y test $i$ correspondiente.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura de una 5-CV\n",
    "    <img src=\"../img/5cv-Copy1.png\" alt=\"5CV\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo el ejercicio anterior, obtendremos una validación cruzada con una estratificación igual a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "wisconsin = sklearn.datasets.load_breast_cancer()\n",
    "wisconsin.keys()\n",
    "wisconsin_data = wisconsin['data']\n",
    "wisconsin_target = wisconsin['target']\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wisconsin_data\n",
    "Y = wisconsin_target\n",
    "\n",
    "K=5\n",
    "acc = []\n",
    "lr = LogisticRegression()\n",
    "skf = StratifiedKFold(n_splits=K, random_state=1)\n",
    "\n",
    "for train_indices, test_indices in skf.split(X,Y):\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
    "    lr.fit(X_train, Y_train)\n",
    "    acc.append(lr.score(X_test, Y_test))\n",
    "    print(acc[-1])\n",
    "\n",
    "print('\\nAcc:{:.4f} Std:{:.4f}'.format(np.mean(acc), np.std(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que con cada semilla se obtienen $(X_t,Y_t)$ y $(X_T,Y_T)$ diferentes, ninguna métrica de rendimiento puede ser considerada como un reflejo único del poder predictivo del modelo. Y, aunque hagamos la media de los valores obtenidos con cada semilla, los conjuntos de train/test que se obtienen por iteración no son disjuntos. Si queremos calcular el poder de generalización con una base de datos de tamaño normal o pequeña, la validación cruzada es una mejor opción.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> Leave-out-out cross-validation o __LOOCV__ es cuando en nuestra K-CV establecemos $K = len(X) - 1$ \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#004D7F\"> <i class=\"fa fa-pencil-square-o\" aria-hidden=\"true\" style=\"color:#113D68\"></i> Ejercicio 2 K-CV</font>\n",
    "\n",
    "Utilizando la base de datos wisconsin, realiza una 10-CV con semilla = 1, utilizando la clase `StratifiedKFold` y su función `split`. Puedes ver la documentación de esta clase <a href=\" http://SciKit Learn-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\"> aquí.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#everything imported in previous example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "K=10\n",
    "lr = LogisticRegression()\n",
    "accuracies = []\n",
    "\n",
    "# Semilla = 1\n",
    "skf = ???\n",
    "for train_indices, test_indices in skf.split(X, Y):\n",
    "    X_t, X_T = X[train_indices], X[test_indices]\n",
    "    Y_t, Y_T = Y[train_indices], Y[test_indices]\n",
    "    lr.???\n",
    "    accuracies.append(lr.score(X_T,Y_T))\n",
    "    print(accuracies[-1])\n",
    "    \n",
    "print('\\nAcc:{:.4f}  Std:{:.4f}'.format(np.mean(accuracies),np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#004D7F\"> 3. Métricas de rendimiento más comunes.</font>\n",
    "\n",
    "Hasta ahora hemos visto 2 métricas como indicadores de la bondad de nuestro modelo: ___accuracy___ o _tasa de aciertos_ en problemas de clasificación, y ___$R^2$___ en regresión. En regresión también es muy común utilizar ___error cuadrático medio___  o _rmse_ (disponible en `sklearn.metrics.mean_squared_error`), que es la función de coste en regresión lineal. \n",
    "\n",
    "---\n",
    "<a id=\"section51\"></a> \n",
    "## <font color=\"#004D7F\">3.1 Matriz de Confusión. </font>\n",
    "\n",
    "En el caso de los problemas de clasificación, hay una gran variedad de métricas que nos serán más o menos útiles según el aspecto de la capacidad predictiva de nuestro modelo que nos interese en un problema dado. Tanto la tasa de aciertos como el resto de métricas que veremos a continuación se pueden calcular a partir de la __matriz de confusion__, la cual expresa los aciertos y fallos para cada valor de la clase.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> Dado un problema de clasificación con una variable clase $C$, la __matriz de confusión__ es una tabla de $|C|x|C|$ celdas, en las que para cada posible valor de la clase se indican la cantidad de casos del conjunto de test que se han clasificado como _Verdaderos Positivos (TP), Verdaderos Negativos (TN), Falsos Negativos (FN)_ y _Falsos Positivos (FP)_. <br> <br>\n",
    "\n",
    "\n",
    "En concreto, en la diagonal principal de la matriz se indican los casos clasificados correctamente. Si la clase es binomial, los TP se refieren a los aciertos de la __clase positiva o de interés__, y los TN a la __clase negativa__. Si la clase es multinomial, los TP se refieren a los aciertos de la __clase de interés__ y los TN a los aciertos en cada una de los otros valores de la clase.\n",
    "</div>\n",
    "\n",
    "En la siguiente tabla se muestra la distribución de una matriz de confusión con clase binomial $C=\\{Sí,No\\}$.\n",
    "<img src=\"../img/matriz confusion-Copy1.png\" alt=\"confusion\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recuento de los aciertos y fallos de las predicciones realizadas por el modelo se hace por supuesto sobre $(X_T,Y_T)$ en el caso de validación _holdout_, y sobre cada $(X_T^i,Y_T^i)$ en el caso de una _K-CV_ o _KxN-CV_, para luego realizar la media de la métrica obtenida en cada conjunto de test _i_.\n",
    "En sklearn, podemos construir la matriz de confusión con la función `sklearn.metrics.confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Xt, XT, Yt, YT = train_test_split(X, Y, test_size=0.3, random_state=1)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(Xt,Yt)\n",
    "predicciones = lr.predict(XT)\n",
    "mc = confusion_matrix(y_true=YT, y_pred=predicciones)\n",
    "clases = np.unique(Y)\n",
    "print(\" Clase Real\\n   {:}    {:}\".format(clases[0],clases[1]))\n",
    "print(mc)\n",
    "print(\"\\nTP={:d} TN={:d}\".format(mc[0,0], mc[1,1]))\n",
    "print(\"FP={:d} FN={:d}\".format(mc[1,0], mc[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En <a href=\"http://SciKit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\"> esta página</a> puedes ver un ejemplo de cómo crear en forma de gráfico una matriz de confusión, por si la quieres compartir en algún informe con aspecto más formal que una simple impresión de texto plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
